{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc15d7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.normalizers import NFKC, Sequence, Lowercase\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.trainers import BpeTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66c04596",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.normalizer = Sequence([\n",
    "    Lowercase()\n",
    "])\n",
    "tokenizer.pre_tokenizer = ByteLevel()\n",
    "tokenizer.decoder = ByteLevelDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2f944cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = BpeTrainer(vocab_size=50000, inital_alphabet=ByteLevel.alphabet(), special_tokens=[\n",
    "            \"<s>\",\n",
    "            \"<pad>\",\n",
    "            \"</s>\",\n",
    "            \"<unk>\",\n",
    "            \"<mask>\"\n",
    "        ])\n",
    "tokenizer.train([\"austen-emma.txt\"], trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "315c301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.mkdir('tokenizer_gpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d938afc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizer_gpt/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4df5d426",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf5be66c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pip install tf-keras --user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ff52a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25498eee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\62331\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2Config, TFGPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c348d4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_gpt = GPT2TokenizerFast.from_pretrained(\"tokenizer_gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "739586d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_gpt.add_special_tokens({\n",
    "  \"eos_token\": \"</s>\",\n",
    "  \"bos_token\": \"<s>\",\n",
    "  \"unk_token\": \"<unk>\",\n",
    "  \"pad_token\": \"<pad>\",\n",
    "  \"mask_token\": \"<mask>\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f39ce83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_gpt.eos_token_id\n",
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e11b31bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 265, 157, 56, 2]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_gpt.encode(\"<s> this is </s>\")\n",
    "# [0, 265, 157, 56, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "051f3570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\62331\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n"
     ]
    }
   ],
   "source": [
    "config = GPT2Config(\n",
    "  vocab_size=tokenizer_gpt.vocab_size,\n",
    "  bos_token_id=tokenizer_gpt.bos_token_id,\n",
    "  eos_token_id=tokenizer_gpt.eos_token_id\n",
    ")\n",
    "model = TFGPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "228d76e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"transformers_version\": \"4.56.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 11750\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "295c1281",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"austen-emma.txt\", \"r\", encoding='utf-8') as f:\n",
    "    content = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "428ae6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_p = []\n",
    "for c in content:\n",
    "    if len(c)>10:\n",
    "        content_p.append(c.strip())\n",
    "content_p = \" \".join(content_p)+tokenizer_gpt.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc52ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_content = tokenizer_gpt.encode(content_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad9bc695",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "block_size = 100\n",
    "BATCH_SIZE = 12\n",
    "BUFFER_SIZE = 1000\n",
    "for i in range(0, len(tokenized_content)):\n",
    "    examples.append(tokenized_content[i:i + block_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "423ae58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [] \n",
    "labels = [] \n",
    "for example in examples: \n",
    "    train_data.append(example[:-1]) \n",
    "    labels.append(example[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08f2b140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07016c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((train_data[:1000], labels[:1000]))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe50636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer], metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da0ac95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\62331\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "83/83 [==============================] - 656s 7s/step - loss: 6.6495 - accuracy: 0.1093\n",
      "Epoch 2/10\n",
      "83/83 [==============================] - 581s 7s/step - loss: 3.5868 - accuracy: 0.3809\n",
      "Epoch 3/10\n",
      "83/83 [==============================] - 587s 7s/step - loss: 1.7779 - accuracy: 0.7161\n",
      "Epoch 4/10\n",
      "83/83 [==============================] - 568s 7s/step - loss: 0.8135 - accuracy: 0.8868\n",
      "Epoch 5/10\n",
      "83/83 [==============================] - 570s 7s/step - loss: 0.4230 - accuracy: 0.9435\n",
      "Epoch 6/10\n",
      "83/83 [==============================] - 580s 7s/step - loss: 0.2607 - accuracy: 0.9667\n",
      "Epoch 7/10\n",
      "83/83 [==============================] - 579s 7s/step - loss: 0.1774 - accuracy: 0.9786\n",
      "Epoch 8/10\n",
      "83/83 [==============================] - 620s 7s/step - loss: 0.1308 - accuracy: 0.9836\n",
      "Epoch 9/10\n",
      "83/83 [==============================] - 629s 8s/step - loss: 0.1010 - accuracy: 0.9871\n",
      "Epoch 10/10\n",
      "83/83 [==============================] - 621s 7s/step - loss: 0.0812 - accuracy: 0.9896\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 10\n",
    "history = model.fit(dataset, epochs=num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b1e495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(start, model):  \n",
    "    input_token_ids = tokenizer_gpt.encode(start, return_tensors='tf')  \n",
    "    output = model.generate(  \n",
    "        input_token_ids,  \n",
    "        max_length = 500,  \n",
    "        num_beams = 5,  \n",
    "        temperature = 0.7,  \n",
    "        no_repeat_ngram_size=2,  \n",
    "        num_return_sequences=1  \n",
    "    )  \n",
    "    return tokenizer_gpt.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00ade96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"  her mother had died too long ago for her to have more than an indistinct remembrance of her caresses; and her place had been supplied by an excellent woman as governess, who had fallen little short of a mother in affection. sixteen years had miss taylor been in mr. woodhouse's family, less as a governess than a friend, very fond of both daughters, but particularly of emma.  between _them_ it was more the intimacy of any restraint; these were the nominal office of authority being now long passed away, they had ceased to hold the mildness of sisters. she had hardly allowed her friend very mutually attached, the shadow of having been living together as friend and the disadvantages which threatened alloy to impose any disagreeable consciousness.--miss taylor's judgment, and emma doing just what she liked; but directed chiefly by his house from a little too much her own way, however, that they did not at present so unperceived, mournful thought as misfortunes sorrow came--a gentle sorrow--but not by any means rank as emma's loss which first sat in the power of this beloved friend that great must be struggled through the actual disparity in health--one to whom she could not have recommended him at all her temper had soon followed isabella's marriage, her father and having rather too well-informed--how she recalled her many acquaintance in consequence by the bride-people gone, on their little children, natural and a disposition to think a valetudinarian all looked up to bear the real evils, indeed of herself; highly esteeming miss-day of their ages (and how was owing here; indeed, though comparatively but sigh over her sister's situation were left to fill the wedding over, with no companion such as few possessed: intelligent, interested in all its separate lawn as usual, great danger of long october and perfect unreserve which hartfield, suitable age, rational or playful to distress or body, till her no prospect of his heart and friend a large debt of hers--and between a third to attach and amuse her pleasant manners; very early) was yet a black morning's work for even before christmas brought the next visit from five years in considering with what self-denying, was now in their being settled in spite of mind or vex speak every thought of had then only half a most affectionate, generous friendship she dearly loved her past kindness-- been mistress of gratitude was some satisfaction and though everywhere beloved her advantages, indulgent father composed of suffering from isabella and their to cheer a long evening must in every promise of its concerns, there.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\" \", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b28804ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'wetson was very good, the bride-people gone, her father and herself were left to dine together, with no prospect of a third to'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(start, model):  \n",
    "    input_token_ids = tokenizer_gpt.encode(start, return_tensors='tf')  \n",
    "    output = model.generate(  \n",
    "        input_token_ids,  \n",
    "        max_length = 30,  \n",
    "        num_beams = 5,  \n",
    "        temperature = 0.7,  \n",
    "        no_repeat_ngram_size=2,  \n",
    "        num_return_sequences=1  \n",
    "    )  \n",
    "    return tokenizer_gpt.decode(output[0])\n",
    "generate(\"wetson was very good\", model)\n",
    "# \"wetson was very good the of her.  the was a, and a of miss taylor; and had, but of a had been a friend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03019cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('my_gpt-2')\n",
    "model.save_pretrained(\"my_gpt-2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b301f122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at my_gpt-2/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_reloaded = TFGPT2LMHeadModel.from_pretrained(\"my_gpt-2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12d5cfc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer_gpt_auto/tokenizer_config.json',\n",
       " 'tokenizer_gpt_auto/special_tokens_map.json',\n",
       " 'tokenizer_gpt_auto/vocab.json',\n",
       " 'tokenizer_gpt_auto/merges.txt',\n",
       " 'tokenizer_gpt_auto/added_tokens.json',\n",
       " 'tokenizer_gpt_auto/tokenizer.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_gpt.save_pretrained(\"tokenizer_gpt_auto/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
